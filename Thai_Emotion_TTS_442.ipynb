{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ntwsut77/Thai_Emotion_TTS/blob/main/Thai_Emotion_TTS_442.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL_kJs4Gp0ub"
      },
      "source": [
        "# üáπüá≠ Emotion-Aware Thai News TTS\n",
        "\n",
        "OFF Project ‚Äî Demo pipeline for **Emotion-Aware Thai Text-to-Speech for News Narration** using:\n",
        "\n",
        "- Whisper (ASR) for optional speech input ‚Üí text\n",
        "- PyThaiNLP for Thai text normalization & sentence segmentation\n",
        "- Rule-based Emotion Classifier (Thai keywords) or manual emotion selection\n",
        "- Massively Multilingual Speech (MMS): Thai Text-to-Speech\n",
        "- Gradio web UI for interactive demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw6ljqEep0ue"
      },
      "source": [
        "## üîÅ System Diagram\n",
        "\n",
        "```text\n",
        "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "          ‚îÇ  Text Input (TH)  ‚îÇ\n",
        "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                    ‚îÇ\n",
        "           OR       ‚îÇ\n",
        "                    ‚ñº\n",
        "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "          ‚îÇ  Audio Input (TH) ‚îÇ\n",
        "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                    ‚îÇ  (librosa / ffmpeg)\n",
        "                    ‚ñº\n",
        "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "          ‚îÇ  Whisper ASR       ‚îÇ  ‚Üí  Thai transcript\n",
        "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                    ‚ñº\n",
        "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "          ‚îÇ  PyThaiNLP         ‚îÇ  ‚Üí  cleaned, segmented text\n",
        "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                    ‚ñº\n",
        "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "          ‚îÇ  Emotion Module                       ‚îÇ\n",
        "          ‚îÇ  - Rule-based Thai emotion classifier ‚îÇ\n",
        "          ‚îÇ  - or manual emotion selection        ‚îÇ\n",
        "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                    ‚ñº\n",
        "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "          ‚îÇ Massively Multilingual Speech (MMS): Thai Text-to-Speech    ‚îÇ\n",
        "          ‚îÇ - Emotion selection           ‚îÇ\n",
        "          ‚îÇ - Speaking rate        ‚îÇ\n",
        "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                    ‚ñº\n",
        "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "          ‚îÇ  Output WAV / UI   ‚îÇ\n",
        "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VpBnnFap0uf"
      },
      "source": [
        "## 1Ô∏è‚É£ Install Dependencies\n",
        "\n",
        "Run this cell once in Colab. It installs the required libraries for ASR, NLP, audio processing, facebook/mms-tts-tha, and Gradio.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObA8vExZp0uf"
      },
      "source": [
        "!pip install -q pandas scikit-learn gradio pythainlp librosa soundfile torch transformers scipy numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5sRJRZpp0ug"
      },
      "source": [
        "## 2Ô∏è‚É£ ASR + NLP + Train TF-IDF + SVM Emotion Classifier\n",
        "\n",
        "- Whisper (via `transformers` pipeline) for Thai speech ‚Üí text\n",
        "- PyThaiNLP for normalization and Thai sentence tokenization\n",
        "- Simple Thai keyword-based emotion classifier for demo purposes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**"
      ],
      "metadata": {
        "id": "FRd12BYNzXUl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2KCH-Cop0ug"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_parquet('/content/corpus.parquet')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auto Label **Emotions**"
      ],
      "metadata": {
        "id": "MWR5XFJazhHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def label_emotion(text):\n",
        "    t=str(text)\n",
        "    if any(k in t for k in ['‡∏î‡∏µ‡πÉ‡∏à','‡∏ä‡∏ô‡∏∞','‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à']): return 'happy'\n",
        "    if any(k in t for k in ['‡πÄ‡∏™‡∏µ‡∏¢‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï','‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢','‡πÄ‡∏®‡∏£‡πâ‡∏≤']): return 'sad'\n",
        "    if any(k in t for k in ['‡πÇ‡∏Å‡∏£‡∏ò','‡∏ß‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡πå','‡πÄ‡∏î‡∏∑‡∏≠‡∏î']): return 'angry'\n",
        "    if any(k in t for k in ['‡∏î‡πà‡∏ß‡∏ô','‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î','‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®']): return 'excited'\n",
        "    return 'news'\n",
        "df['emotion']=df['text'].apply(label_emotion)\n",
        "df['emotion'].value_counts()"
      ],
      "metadata": {
        "id": "F2A_jcJtzkpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train TF-IDF + SVM Emotion Classifier**"
      ],
      "metadata": {
        "id": "NGsir4r0zz8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['emotion'], test_size=0.2, random_state=42)\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=20000)\n",
        "X_train_vec = tfidf.fit_transform(X_train)\n",
        "X_test_vec = tfidf.transform(X_test)\n",
        "\n",
        "clf = LinearSVC(class_weight='balanced')\n",
        "clf.fit(X_train_vec, y_train)\n",
        "\n",
        "print('Accuracy:', clf.score(X_test_vec, y_test))"
      ],
      "metadata": {
        "id": "iQQ8ExqMziq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whisper + **Preprocessing**"
      ],
      "metadata": {
        "id": "GyW1Tl6Wz9sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import librosa\n",
        "from pythainlp.util import normalize\n",
        "# ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á import word_tokenize ‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏Ñ‡∏£‡∏±‡∏ö\n",
        "\n",
        "# 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Whisper (‡πÉ‡∏™‡πà chunk_length_s=30 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô Error ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏¢‡∏≤‡∏ß)\n",
        "asr = pipeline('automatic-speech-recognition',\n",
        "               model='openai/whisper-large-v3',\n",
        "               device=0,\n",
        "               chunk_length_s=30)\n",
        "\n",
        "def speech_to_text(path):\n",
        "    # ‡∏™‡πà‡∏á path ‡πÉ‡∏´‡πâ pipeline ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏¢‡∏≤‡∏ß)\n",
        "    return asr(path, batch_size=8)['text']\n",
        "\n",
        "def preprocess(txt):\n",
        "    # 1. ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ input ‡πÄ‡∏õ‡πá‡∏ô None\n",
        "    if not txt or not isinstance(txt, str):\n",
        "        return \"\"\n",
        "\n",
        "    # 2. Normalize ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏Ç‡∏¢‡∏∞, ‡∏™‡∏£‡∏∞‡∏•‡∏≠‡∏¢)\n",
        "    txt = normalize(txt.strip())\n",
        "\n",
        "    # 3. üî• ‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô word_tokenize ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ‡πÄ‡∏•‡∏¢ üî•\n",
        "    # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ï‡∏£‡∏á‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ TTS ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥\n",
        "    return txt"
      ],
      "metadata": {
        "id": "zdcVD9N0z_Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miEUHDQhp0ug"
      },
      "source": [
        "## 3Ô∏è‚É£  Text-to-Speech (TTS)\n",
        "Massively Multilingual Speech (MMS): Thai Text-to-Speech\n",
        "\n",
        "### üîê model detail\n",
        "VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech) is an end-to-end speech synthesis model that predicts a speech waveform conditional on an input text sequence. It is a conditional variational autoencoder (VAE) comprised of a posterior encoder, decoder, and conditional prior.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m78_r_ngp0ug"
      },
      "source": [
        "from transformers import VitsModel, AutoTokenizer\n",
        "import torch, numpy as np\n",
        "import scipy.io.wavfile as wavfile\n",
        "\n",
        "MODEL_ID = \"facebook/mms-tts-tha\"\n",
        "tts_model = VitsModel.from_pretrained(MODEL_ID)\n",
        "tts_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROSODY_MAP = {\n",
        "    # ‡∏Ç‡πà‡∏≤‡∏ß: ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏õ‡∏Å‡∏ï‡∏¥, ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ô‡∏¥‡πà‡∏á‡πÜ (Safe Zone)\n",
        "    'news':    {'speed': 1.0,  'noise': 0.5},\n",
        "\n",
        "    # ‡∏î‡∏µ‡πÉ‡∏à: ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢, ‡∏°‡∏µ‡∏•‡∏π‡∏Å‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì\n",
        "    'happy':   {'speed': 1.1,  'noise': 0.7},\n",
        "\n",
        "    # ‡πÄ‡∏®‡∏£‡πâ‡∏≤: ‡∏ä‡πâ‡∏≤‡∏•‡∏á (‡πÅ‡∏ï‡πà‡∏≠‡∏¢‡πà‡∏≤‡∏¢‡∏≤‡∏ô‡∏à‡∏ô‡∏ô‡πà‡∏≤‡∏£‡∏≥‡∏Ñ‡∏≤‡∏ç), ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏ï‡πà‡∏≥\n",
        "    'sad':     {'speed': 0.89, 'noise': 0.3},\n",
        "\n",
        "    # ‡πÇ‡∏Å‡∏£‡∏ò: ‡πÄ‡∏£‡πá‡∏ß ‡∏Å‡∏£‡∏∞‡∏ä‡∏≤‡∏Å, ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÅ‡∏Ç‡πá‡∏á (Noise ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢)\n",
        "    'angry':   {'speed': 1.3, 'noise': 0.8},\n",
        "\n",
        "    # ‡∏ï‡∏∑‡πà‡∏ô‡πÄ‡∏ï‡πâ‡∏ô: ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å, ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏™‡∏π‡∏á (Noise ‡∏≠‡∏¢‡πà‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô 0.85 ‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÅ‡∏ï‡∏Å)\n",
        "    'excited': {'speed': 1,  'noise': 0.85},\n",
        "}"
      ],
      "metadata": {
        "id": "ictxIK760kdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import wavfile\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def cloud_tts(text, emotion):\n",
        "    # 1. ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ Config ‡∏ï‡∏≤‡∏°‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å\n",
        "    cfg = PROSODY_MAP.get(emotion, PROSODY_MAP[\"news\"])\n",
        "\n",
        "    # 2. üî• ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà Error:\n",
        "    # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤ forward() ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏¢‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏™‡πà Config ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÅ‡∏ó‡∏ô\n",
        "    tts_model.config.speaking_rate = cfg['speed']  # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß\n",
        "    tts_model.config.noise_scale = cfg['noise']    # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏ß‡∏µ‡πà‡∏¢‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á\n",
        "\n",
        "    # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Input\n",
        "    inputs = tts_tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "    # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            # ‡∏•‡∏ö noise_scale/length_scale ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô config ‡πÅ‡∏•‡πâ‡∏ß\n",
        "            output = tts_model(**inputs)\n",
        "\n",
        "        audio = output.waveform[0].numpy()\n",
        "\n",
        "        # 5. Normalize ‡πÄ‡∏™‡∏µ‡∏¢‡∏á (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÅ‡∏ï‡∏Å)\n",
        "        max_val = np.max(np.abs(audio))\n",
        "        if max_val > 0:\n",
        "            audio = audio / max_val * 32767\n",
        "\n",
        "        audio = audio.astype(np.int16)\n",
        "\n",
        "        # 6. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå\n",
        "        output_file = \"out.wav\"\n",
        "        wavfile.write(output_file, tts_model.config.sampling_rate, audio)\n",
        "\n",
        "        return output_file\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in cloud_tts: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "aU8v8Arw0mWu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS2Tachxp0uh"
      },
      "source": [
        "## 4Ô∏è‚É£ Gradio Web App\n",
        "\n",
        "- Input: Thai text **or** uploaded Thai speech audio\n",
        "- Options:\n",
        "  - Automatic emotion detection from text (rule-based)\n",
        "  - Manual override of emotion\n",
        "  - Speaker (voice) selection\n",
        "  - Speaking rate and pitch sliders (for fine-tuning prosody)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia0dPYjEp0uh"
      },
      "source": [
        "import gradio as gr\n",
        "\n",
        "def run(text_input, audio_input, use_auto, manual_emo):\n",
        "    try:\n",
        "        print(\"--- NEW RUN ---\")\n",
        "        final_text = \"\"\n",
        "\n",
        "        # 1. ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏´‡∏° (‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏Å‡πà‡∏≠‡∏ô)\n",
        "        if audio_input is not None:\n",
        "            print(\"üé§ Processing Audio...\")\n",
        "\n",
        "            # ‡∏î‡∏∂‡∏á Path ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢\n",
        "            if isinstance(audio_input, dict):\n",
        "                audio_path = audio_input.get(\"name\") or audio_input.get(\"data\")\n",
        "            else:\n",
        "                audio_path = audio_input\n",
        "\n",
        "            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Whisper\n",
        "            final_text = speech_to_text(audio_path)\n",
        "            print(f\"üìù ASR Result: '{final_text}'\")\n",
        "\n",
        "        # 2. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏™‡∏µ‡∏¢‡∏á ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Text ‡∏ó‡∏µ‡πà‡∏û‡∏¥‡∏°‡∏û‡πå‡∏°‡∏≤\n",
        "        else:\n",
        "            final_text = text_input\n",
        "            print(f\"‚å®Ô∏è Using Text Input: '{final_text}'\")\n",
        "\n",
        "        # 3. üî• ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏° ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏•‡∏¢ (‡∏Å‡∏±‡∏ô TTS ‡∏û‡∏±‡∏á)\n",
        "        if not final_text or final_text.strip() == \"\":\n",
        "            msg = \"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏û‡∏π‡∏î ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡∏≠‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ (Silence detected)\"\n",
        "            print(msg)\n",
        "            return None, msg, \"Error\"\n",
        "\n",
        "        # 4. Preprocess (‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥)\n",
        "        processed_text = preprocess(final_text)\n",
        "        print(f\"‚úÇÔ∏è Processed: '{processed_text}'\")\n",
        "\n",
        "        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏≠‡∏µ‡∏Å‡∏£‡∏≠‡∏ö‡πÄ‡∏ú‡∏∑‡πà‡∏≠ preprocess ‡πÅ‡∏•‡πâ‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏≤‡∏¢‡∏´‡∏°‡∏î (‡πÄ‡∏ä‡πà‡∏ô ‡∏°‡∏µ‡πÅ‡∏ï‡πà‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå)\n",
        "        if not processed_text or processed_text.strip() == \"\":\n",
        "             return None, \"Error: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏µ‡πÅ‡∏ï‡πà‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ\", \"Error\"\n",
        "\n",
        "        # 5. ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå\n",
        "        if use_auto:\n",
        "            emo = clf.predict(tfidf.transform([processed_text]))[0]\n",
        "        else:\n",
        "            emo = manual_emo\n",
        "        print(f\"üòä Emotion: {emo}\")\n",
        "\n",
        "        # 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á (TTS)\n",
        "        # ‡∏ñ‡πâ‡∏≤‡∏û‡∏±‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏ß‡πá‡∏ö‡∏Ñ‡πâ‡∏≤‡∏á ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡πÇ‡∏ä‡∏ß‡πå Error\n",
        "        audio_path = cloud_tts(processed_text, emo)\n",
        "\n",
        "        if audio_path is None:\n",
        "             return None, \"Error: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (TTS Failed)\", emo\n",
        "\n",
        "        print(\"‚úÖ Success!\")\n",
        "        return audio_path, processed_text, emo\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"üî• CRITICAL ERROR: {e}\")\n",
        "        return None, f\"System Error: {str(e)}\", \"Crash\"\n",
        "\n",
        "# --- UI Setup ---\n",
        "demo = gr.Interface(\n",
        "    fn=run,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Text Input (‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)\"),\n",
        "        gr.Audio(label=\"Voice Input (‡∏≠‡∏±‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á/‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î)\", type=\"filepath\"),\n",
        "        gr.Checkbox(label=\"Auto Emotion\", value=True),\n",
        "        gr.Dropdown([\"news\",\"happy\",\"sad\",\"angry\",\"excited\"], label=\"Manual Emotion\", value=\"news\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Audio(label=\"Generated Audio\"),\n",
        "        gr.Textbox(label=\"Result / Status\"),\n",
        "        gr.Textbox(label=\"Detected Emotion\")\n",
        "    ],\n",
        "    title=\"Thai Emotion TTS\",\n",
        "    description=\"‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡πÅ‡∏•‡∏∞ Error ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß\",\n",
        "    allow_flagging=\"manual\",\n",
        "    flagging_dir=\"my_flagged_data\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}